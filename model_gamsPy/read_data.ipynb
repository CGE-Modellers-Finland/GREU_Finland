{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import gamspy as gp\n",
    "import gams.core.numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAMSPy set-up + introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe data we read in this file is from several different .xlsx sheets. \\nThe script runs reasonably efficiently with a few exceptions, it does however rely quite heavily on the pandas-package atm.\\nIn terms of robustness, I rely quite heavily on the naming conventions and datastructure in the datasheets.\\nThe names are not extracted, but typed which comes with some risk should those conventions change.\\nThe gp.Set(), gp.Parameter()-functions that I rely on for datatransfer are also snesitive to changes in the data-structure, that is domain-names must be entered in the \"correct\" order, especially when relying on the\\nvery efficient domain_forwarding for populating sets.\\nInitially I define a number of dictionaries. They are mostly self-explanatory, but the ones that are not are adressed immediately below.\\nI see no way around having to do this, as the data here is read into sets and parametres that must correpond to sets in the model for it to run.\\nFor any other application where data is not in exactly the same format will likely mean having to adapt these.\\nFurthermore, the order of sets is not trivial. Therefore I also must reorder columns for compatibility with the model.\\nThe easiest way to get this correct, I imagine is to look at the actual parametres being exported when gp.Parameter is called.\\ngp.Parameter takes an input called domain, which is the sets on which the parameter is defined, and the order is the order which corresponds to the order in the model.\\nColumns are not required to have the same header as the set to which it corresponds, but the data being exported is required to have a column titled \"level\" which is the values-column.\\n\\nEnable copy_on_write, this will be default in upcoming pandas-version and some methods will become depreciated (including a personal favorite of mine df[col].method(blabla, inplace=True)),\\nspecifically chained assignments and operating on some views of objects which in fringe cases can lead to misassignments, in addition, methods that I use throughout (df.drop,df.rename) are modified \\nto return views until modified further since these do not require copies of data resulting in overall performance improving. Excluding all this, copy_on_write will become default anyways, \\nmeaning that failing to comply with the required specifications for this mode now, will eventually cause the code to fail and users will have to rely on legacy-versions of pandas, or modify the script locally.\\nIf you must use deprecated methods, the latest version of pandas that supports chained assignments and operating on all views is 2.2.3\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=gp.Container()\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "'''\n",
    "\"Clean version\". No experimentation or debugging here\n",
    "\n",
    "The data we read in this file is from several different .xlsx sheets. \n",
    "The script runs reasonably efficiently with a few exceptions, it does however rely quite heavily on the pandas-package atm.\n",
    "In terms of robustness, I rely quite heavily on the naming conventions and datastructure in the datasheets.\n",
    "The names are not extracted, but typed which comes with some risk should those conventions change.\n",
    "The gp.Set(), gp.Parameter()-functions that I rely on for datatransfer are also snesitive to changes in the data-structure, that is domain-names must be entered in the \"correct\" order, especially when relying on the\n",
    "very efficient domain_forwarding for populating sets.\n",
    "Initially I define a number of dictionaries. They are mostly self-explanatory, but the ones that are not are adressed immediately below.\n",
    "I see no way around having to do this, as the data here is read into sets and parametres that must correpond to sets in the model for it to run.\n",
    "For any other application where data is not in exactly the same format will likely mean having to adapt these.\n",
    "Furthermore, the order of sets is not trivial. Therefore I also must reorder columns for compatibility with the model.\n",
    "The easiest way to get this correct, I imagine is to look at the actual parametres being exported when gp.Parameter is called.\n",
    "gp.Parameter takes an input called domain, which is the sets on which the parameter is defined, and the order is the order which corresponds to the order in the model.\n",
    "Columns are not required to have the same header as the set to which it corresponds, but the data being exported is required to have a column titled \"level\" which is the values-column.\n",
    "\n",
    "Enable copy_on_write, this will be default in upcoming pandas-version and some methods will become depreciated (including a personal favorite of mine df[col].method(blabla, inplace=True)),\n",
    "specifically chained assignments and operating on some views of objects which in fringe cases can lead to misassignments, in addition, methods that I use throughout (df.drop,df.rename) are modified \n",
    "to return views until modified further since these do not require copies of data resulting in overall performance improving. Excluding all this, copy_on_write will become default anyways, \n",
    "meaning that failing to comply with the required specifications for this mode now, will eventually cause the code to fail and users will have to rely on legacy-versions of pandas, or modify the script locally.\n",
    "If you must use deprecated methods, the latest version of pandas that supports chained assignments and operating on all views is 2.2.3\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dictionaries for mapping from raw data onto GR-set members'''\n",
    "metadata_dicte=pd.read_excel(r'data\\metadata.xlsx',sheet_name='energy_products')\n",
    "dict_e = dict(zip(metadata_dicte['product_greu'], metadata_dicte['product_greu_txt']))\n",
    "\n",
    "dict_transaction={'transmis_loss':'transmission_losses','cons_inter':'input_in_production','cons_hh':'household_consumption','import':'imports','invent_change':'inventory'}\n",
    "\n",
    "dict_ebalitems={'ws_marg':'EAV','ret_marg':'DAV','basic':'BASE','co2_xbio':'co2ubio','co2_eq':'co2e','co2_bio':'co2bio'}\n",
    "\n",
    "dict_a={'tax_products':'TaxSub','tax_vat':'Moms','emp_comp':'SalEmpl','subs_other_production':'OthSubs','tax_other_production':'OthTax','gross_surplus':'OvProd'} #turister\n",
    "\n",
    "io_inv_dict={'invest_build':'iB','invest_other':'iM','invest_trans':'iT'}\n",
    "\n",
    "fixed_assets_dict={'N11P':'iM','N1121':'iB','N1122_3':'iB','N1131':'iT','N115':'iM','N117':'iM','N111':'iB'}\n",
    "'''since above is not self-explanatory.\n",
    "N11P is \"ICT-equipment, other machinery and stock and weapons systems\"\n",
    "N1122_3 is \"facilities\"\n",
    "N1131 is \"means of transport\"\n",
    "N115 is \"stock of animals\"\n",
    "N117 is \"intellectual rights\"\n",
    "N111 is \"housing\"\n",
    "'''\n",
    "'''List of model years'''\n",
    "t_list=[i for i in range(1980, 2100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### energy_and_emissions.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonenergyemissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_energy_emissions=pd.read_excel(r'data\\non_energy_emissions.xlsx',keep_default_na=True)\n",
    "non_energy_emissions.set_index(['year','bal','flow','indu'],inplace=True)\n",
    "#stack to obtain a column of emission-types\n",
    "non_energy_emissions=non_energy_emissions.stack().to_frame(name='level')\n",
    "non_energy_emissions.dropna(inplace=True)\n",
    "#impose ebalitems name\n",
    "non_energy_emissions.index.rename(['year','bal','transaction','d','ebalitems'],inplace=True)\n",
    "non_energy_emissions.reset_index(inplace=True)\n",
    "non_energy_emissions.drop(columns='bal',inplace=True)\n",
    "\n",
    "non_energy_emissions.replace({'transaction':dict_transaction,'ebalitems':dict_ebalitems},inplace=True)\n",
    "\n",
    "# set column order\n",
    "non_energy_emissions = non_energy_emissions[['ebalitems', 'transaction', 'd', 'year', 'level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_and_emissions=pd.read_excel(r'data\\energy_and_emissions.xlsx',keep_default_na=True)\n",
    "\n",
    "'''rename coslumns for compatibility with data loading'''\n",
    "energy_and_emissions.rename(columns={'indu':'d','product':'e','purp':'es','flow':'transaction'},inplace=True)\n",
    "energy_and_emissions.set_index(['year','bal','transaction','d','es','e'],drop=True,inplace=True)\n",
    "\n",
    "energy_and_emissions=energy_and_emissions.stack().to_frame(name='level')\n",
    "energy_and_emissions.index.set_names(['year','bal','transaction','d','es','e','ebalitems'],inplace=True)\n",
    "energy_and_emissions.reset_index(inplace=True)\n",
    "energy_and_emissions.fillna({'es':'unspecified'},inplace=True)\n",
    "energy_and_emissions.replace({'transaction':dict_transaction,'ebalitems':dict_ebalitems},inplace=True)\n",
    "\n",
    "\n",
    "'''replace those pesky nans with a target-specific label'''\n",
    "energy_and_emissions['d'] = energy_and_emissions.apply(lambda row: 'xOth' if pd.isna(row['d']) and row['transaction'] == 'export' \n",
    "                               else ('invt' if pd.isna(row['d']) and row['transaction'] == 'inventory' \n",
    "                                     else ('tl' if pd.isna(row['d']) and row['transaction'] == 'transmission_losses' \n",
    "                                           else ('natural_input' if pd.isna(row['d']) and row['transaction'] == 'nat_input'\n",
    "                                                 else ('residual' if pd.isna(row['d']) and row['transaction'] == 'res_input'\n",
    "                                                       else ('19000' if pd.isna(row['d']) and row['transaction']=='imports'\n",
    "                                                            else row['d']))))), axis=1)\n",
    "\n",
    "energy_and_emissions['e']=energy_and_emissions['e'].replace(dict_e)\n",
    "\n",
    "# set column order\n",
    "energy_and_emissions = energy_and_emissions[['ebalitems', 'transaction', 'd', 'es', 'e', 'year', 'level']]\n",
    "'''retrieve unique values from the e-column to populate e and its superset out.\n",
    "I add some records manually that are explicitly called in the model, but are not present in data.\n",
    "'''\n",
    "e_vals=list(set(energy_and_emissions[['e']].values.flatten()))\n",
    "out_vals=e_vals.copy()\n",
    "out_vals.extend(['out_other','WholeAndRetailSaleMarginE','Natural gas (Extraction)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IO-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''gampy does not support domain forwarding from set to set.\n",
    "To construct a superset of IO-sectors we must initially populate the superset, then provide it as domain for subsets, which we can then populate using the tried and tested domain_forwarding method.\n",
    "To populate the superset, we must load data from the io-spreadsheets.\n",
    "On the data:\n",
    "The Danish IO-tables are received in long-format and matrix-format. I read the long-format for convenience, but the content of the data is better understood when veiwing in matrix-format.\n",
    "The columns consist of demand-components, which can be either sectors of the economy, export, investment or some final-demand component such as food or housing.\n",
    "The rows consist of inputs, and can be subdivided into three categories, domestic production, import and primary inputs.\n",
    "The primary inputs are taxes, subsidies, employee compensation and the likes.\n",
    "Imported and domestically produced inputs are industry-outputs such that a number in any entry of the IO-matrix can be read as the supply from the row-index to the column-index.\n",
    "To make this data compatible with the model, we must first inform the model how to interpret the indices. We do this by defining sets.\n",
    "For instance the set \"i\" is the set of sectors in the model, we can therefore say that the rows of the import- and domestically produced part of the io-table, consist of members of i. \n",
    "The set d is the set of demand-components and corresponds to the columns of the io-tables.\n",
    "By creating separate variables for imported and domestically produced supply from the io-table, we can define a variable like vIO_y, and define in on [i,d,t], which we can then read as \n",
    "domestic sector i's supply to demand component d at year t. Atm, I think the easiest to follow in the extremely likely event that data is not in the exact same format, is the current version\n",
    "in which I explicitly label columns in the dataframes according to their GR-set-counterparts. \n",
    "It is important not to mess with the sets, as they are called explicitly in the model, always by name and on occasion we also refer to specific elements.\n",
    "The consequence of this is that names have to be inserted here. This will almost certainly need to be edited for other sources of data.\n",
    "I try to make apparent from the naming throughout where columns and rows are meant to end up in the model which\n",
    "I suspect will ultimately be more convenient for adapting this code to other datasets than attempting to accomplish the\n",
    "highest possible degree of automization.\n",
    "'''\n",
    "#energy\n",
    "io_energy=pd.read_excel(r'data\\io_energy_long_format.xlsx',keep_default_na=True)\n",
    "io_energy_forlater=io_energy.copy(deep=True)\n",
    "io_energy.rename(columns={'row_l2':'i','col_l1':'DELETE','col_l2':'d','value':'level'},inplace=True)\n",
    "\n",
    "#regular\n",
    "io=pd.read_excel(r'data\\io_long_format.xlsx')\n",
    "io_forlater=io.copy(deep=True)\n",
    "io.rename(columns={'row_l2':'i','col_l1':'DELETE','col_l2':'d','value':'level'},inplace=True)\n",
    "\n",
    "'''fill nans based on previous entries. In Danish dataset, col_l1 contains supercategories for col_l2.\n",
    "Meanwhile, we use col_l2, which contains useful categories such as sectors (as opposed to cons_inter). \n",
    "Categories which do not have a subcategory, such as cons_publ (public consumption) have an empty cell at col_l2.\n",
    "In this program I create the set of demand components d from its members, so in order to represent the demand components that do\n",
    "not have multiple subcategories, I run the lines below which fills the appropriate GR-demand component based on the supercategory, i.e.\n",
    "if row10 supercategory is export, replace NaN in the demand component column of row10 with xOth.\n",
    "'''\n",
    "io['d'] = io.apply(lambda row: 'xOth' if pd.isna(row['d']) and row['DELETE'] == 'export' \n",
    "                               else ('invt' if pd.isna(row['d']) and row['DELETE'] == 'invent_change' \n",
    "                                     else ('g' if pd.isna(row['d']) and row['DELETE'] == 'cons_publ' \n",
    "                                           else ('iB' if pd.isna(row['d']) and row['DELETE'] == 'invest_build'\n",
    "                                                 else ('iT' if pd.isna(row['d']) and row['DELETE'] == 'invest_trans'\n",
    "                                                       else ('iM' if pd.isna(row['d']) and row['DELETE']=='invest_other'\n",
    "                                                            else row['d']))))), axis=1)\n",
    "\n",
    "io_energy['d'] = io_energy.apply(lambda row: 'xOth' if pd.isna(row['d']) and row['DELETE'] == 'export' \n",
    "                               else ('invt' if pd.isna(row['d']) and row['DELETE'] == 'invent_change' \n",
    "                                     else ('g' if pd.isna(row['d']) and row['DELETE'] == 'cons_publ' \n",
    "                                           else ('iB' if pd.isna(row['d']) and row['DELETE'] == 'invest_build'\n",
    "                                                 else ('iT' if pd.isna(row['d']) and row['DELETE'] == 'invest_trans'\n",
    "                                                       else ('iM' if pd.isna(row['d']) and row['DELETE']=='invest_other'\n",
    "                                                            else row['d']))))), axis=1)\n",
    "\n",
    "'''Editors note: At the time of writing, the model is not equipped to handle the distinction between con_hh and cons_hh_foreign, therefore\n",
    "for the time being we simply add them together, I do this by creating a boolean mask to identify rows where the aforementioned supercategory-column \n",
    "contains the string cons_hh, which in the GR dataset is both cons_hh and cons_hh_foreign_tou. I then build a dataframe of the entries with only the members of\n",
    "the original frame(s) that has has cons_hh or cons_hh_foreign_tou as supercategory. I then group entries in this dataset based on entries in the other columns (except of course level) and add them together.\n",
    "The rows that were selected for this process is then dropped from the original frame(s) and the aggregated rows are added back.\n",
    "'''\n",
    "mask=io[\"DELETE\"].str.contains(\"cons_hh\")\n",
    "mask_ene = io_energy[\"DELETE\"].str.contains(\"cons_hh\")\n",
    "io_agg = io[mask].groupby(io.columns.difference([\"DELETE\",\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "io_energy_agg= io_energy[mask_ene].groupby(io_energy.columns.difference([\"DELETE\",\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "'''drop rows that had been aggregated'''\n",
    "io = io[~mask]\n",
    "io_energy=io_energy[~mask_ene]\n",
    "'''drop DELETE'''\n",
    "io.drop(columns=['DELETE'],inplace=True)\n",
    "io_energy.drop(columns=['DELETE'],inplace=True)\n",
    "'''reconcat'''\n",
    "io=pd.concat([io,io_agg])\n",
    "io_energy=pd.concat([io_energy,io_energy_agg])\n",
    "'''add t's'''\n",
    "io=pd.concat([io,pd.DataFrame(columns=['t'],index=io.index,data=2020)],axis=1)\n",
    "io_energy=pd.concat([io_energy,pd.DataFrame(columns=['t'],index=io_energy.index,data=2020)],axis=1)\n",
    "'''reorder for consistency with GR-variables'''\n",
    "io=io[['row_l1','i', 'd', 't', 'level']]\n",
    "io_energy=io_energy[['row_l1','i', 'd', 't', 'level']]\n",
    "'''disentagle'''\n",
    "io_y=io[io['row_l1']=='production']\n",
    "io_m=io[io['row_l1']=='import']\n",
    "io_a=io[io['row_l1']=='prim_input']\n",
    "io_ene_y=io_energy[io_energy['row_l1']=='production']\n",
    "io_ene_m=io_energy[io_energy['row_l1']=='import']\n",
    "io_ene_a=io_energy[io_energy['row_l1']=='prim_input']\n",
    "'''drop columns'''\n",
    "io_y=io_y.drop(columns=['row_l1'])\n",
    "io_m=io_m.drop(columns=['row_l1'])\n",
    "io_ene_a=io_ene_a.drop(columns=['row_l1'])\n",
    "io_ene_y=io_ene_y.drop(columns=['row_l1'])\n",
    "io_ene_m=io_ene_m.drop(columns=['row_l1'])\n",
    "io_a=io_a.drop(columns=['row_l1'])\n",
    "'''apply a_dict'''\n",
    "io_a.replace({'i':dict_a},inplace=True)\n",
    "io_ene_a.replace({'i':dict_a},inplace=True)\n",
    "\n",
    "io_a=io_a.groupby(['d','i','t'],as_index=False).agg({'level':'sum'})\n",
    "io_combined_a = pd.concat([io_a, io_ene_a]).groupby(io_a.columns.difference([\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "'''add energy to non-energy for vIO_{y,m,a}'''\n",
    "io_combined_y = pd.concat([io_y, io_ene_y]).groupby(io_y.columns.difference([\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "io_combined_m=pd.concat([io_m, io_ene_m]).groupby(io_m.columns.difference([\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "io_combined_a=io_combined_a.groupby(['d','i','t'],as_index=False).agg({'level':'sum'})\n",
    "'''adding tot by turning on below. Note, then 'tot' should also be an element of d yabish'''\n",
    "'''io_y_agg=io_y.groupby('d',as_index=False)['level'].sum()\n",
    "io_y_agg.insert(0,'i','tot')\n",
    "io_y_agg.insert(2,'t','2020')\n",
    "io_y=pd.concat([io_y,io_y_agg])\n",
    "io_m_agg=io_m.groupby('d',as_index=False)['level'].sum()\n",
    "io_m_agg.insert(0,'i','tot')\n",
    "io_m_agg.insert(2,'t','2020')\n",
    "io_m=pd.concat([io_m,io_m_agg])\n",
    "io_a_agg=io_a.groupby('d',as_index=False)['level'].sum()\n",
    "io_a_agg.insert(0,'i','tot')\n",
    "io_a_agg.insert(2,'t','2020')\n",
    "io_a=pd.concat([io_a,io_a_agg])\n",
    "io_combined_y_agg=io_combined_y.groupby('d',as_index=False)['level'].sum()\n",
    "io_combined_y_agg.insert(0,'i','tot')\n",
    "io_combined_y_agg.insert(2,'t','2020')\n",
    "io_combined_y=pd.concat([io_combined_y,io_combined_y_agg])\n",
    "io_combined_m_agg=io_combined_m.groupby('d',as_index=False)['level'].sum()\n",
    "io_combined_m_agg.insert(0,'i','tot')\n",
    "io_combined_m_agg.insert(2,'t','2020')\n",
    "io_combined_m=pd.concat([io_combined_m,io_combined_m_agg])'''\n",
    "\n",
    "'''change order for GR-compatibility'''\n",
    "io_a=io_a[['i','d','t','level']]\n",
    "io_combined_a=io_combined_a[['i','d','t','level']]\n",
    "\n",
    "'''A list of elements in i'''\n",
    "i_elements =list(set(io_y['i']).union(set(io_m['i'])))\n",
    "i_re_elements=[i+'_re' for i in i_elements]\n",
    "'''there is another set \"rx\" whose elements are those of i, then there is a set which serves the purpose of mapping between rx and re.\n",
    "Below I construct a list of tuples on the form (x,x_re) where x ∈ re to populate this set.\n",
    "'''\n",
    "sorted_i_re_elements=sorted(i_re_elements,key=lambda x: i_elements.index(x.split('_')[0]))\n",
    "rx2re_list=list(zip(i_elements,sorted_i_re_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''IO investment from here'''\n",
    "\n",
    "io_inv=pd.read_excel(r'data\\io_invest_long_format.xlsx',keep_default_na=True)\n",
    "io_inv.rename(columns={'col':'i','invest_group':'k','value':'level'},inplace=True)\n",
    "'''apply dict for GR-compatible codes'''\n",
    "io_inv['k']=io_inv['k'].replace(io_inv_dict)\n",
    "\n",
    "'''atm we do not care abt. \"sender\" of capital, just building qI_k_i'''\n",
    "io_inv_qI_k_i=io_inv.copy(deep=True)\n",
    "\n",
    "io_inv_qI_k_i.drop(columns=['row_l1','row_l2'],inplace=True)\n",
    "io_inv_qI_k_i=io_inv_qI_k_i[['k','i','year','level']]\n",
    "'''aggregate'''\n",
    "io_inv_qI_k_i_agg=io_inv_qI_k_i.groupby(['k','i','year'],as_index=False).agg({'level':'sum'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demand-side based io incl. employee compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''build qL'''\n",
    "io_qRxE=io_forlater[io_forlater['col_l1']=='cons_inter']\n",
    "\n",
    "io_qRxE=io_qRxE.rename(columns={'col_l2':'i'})\n",
    "\n",
    "io_qRxE=io_qRxE.replace({'row_l2':dict_a})\n",
    "\n",
    "io_l=io_qRxE[io_qRxE['row_l2']=='SalEmpl']\n",
    "io_l_s=io_l.groupby(['i','year'],as_index=False).agg({'value':'sum'})\n",
    "'''\n",
    "Above is the total value of labor from employees. \n",
    "This must be upscaled by the contribution of independents\n",
    "Wages of independents are somewhat complicated. We calculate them by:\n",
    "wages_employed * hours_independents / hours_employed\n",
    "and add this to the existing wage sum.\n",
    "Below reads the data required to compute the expression above:\n",
    "'''\n",
    "employed_fullset=pd.read_excel(r'data\\employed.xlsx',keep_default_na=True)\n",
    "employed_fullset.rename(columns={'indu':'i'},inplace=True)\n",
    "employed_employees=employed_fullset[employed_fullset['type']=='employees'][['i', 'hours']]\n",
    "employed_independent=employed_fullset[employed_fullset['type']=='self-employed'][['i', 'hours']]\n",
    "\n",
    "wagesum=io_l_s[['i','value']]\n",
    "# set index\n",
    "wagesum.set_index('i',inplace=True)\n",
    "employed_employees.set_index('i',inplace=True)\n",
    "employed_employees.rename(columns={'hours':'value'},inplace=True)\n",
    "employed_independent.set_index('i',inplace=True)\n",
    "employed_independent.rename(columns={'hours':'value'},inplace=True)\n",
    "\n",
    "#make sure that all indices are valid\n",
    "wagesum_index=wagesum.index.union(employed_employees.index).union(employed_independent.index)\n",
    "wagesum =wagesum.reindex(wagesum_index, fill_value=0)\n",
    "employed_employees=employed_employees.reindex(wagesum_index, fill_value=0)\n",
    "employed_independent=employed_independent.reindex(wagesum_index, fill_value=0)\n",
    "\n",
    "#calculate actual wage compensation\n",
    "wagesum_=wagesum+wagesum*employed_independent/employed_employees\n",
    "wagesum_.reset_index(inplace=True)\n",
    "wagesum_.rename(columns={'value':'level'},inplace=True)\n",
    "#insert t\n",
    "wagesum_with_t=pd.concat([wagesum_,pd.DataFrame(index=wagesum_.index,columns=['t'],data=2020)],axis=1)\n",
    "#reorder columns\n",
    "wagesum_with_t=wagesum_with_t[['i','t','level']]\n",
    "#total number of employed including independents\n",
    "nemployed_frame = pd.DataFrame(columns=['t', 'level'], data=[[2020, employed_fullset['employed'].sum()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capital, fixed assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_assets=pd.read_excel(r'data\\fixed_assets.xlsx',keep_default_na=True)\n",
    "'''map onto gr-codes'''\n",
    "fixed_assets.replace({'asset':fixed_assets_dict},inplace=True)\n",
    "\n",
    "fixed_assets.rename(columns={'asset':'k','indu':'i','value':'level'},inplace=True)\n",
    "'''reorder columns'''\n",
    "fixed_assets=fixed_assets[['k','i','year','level']]\n",
    "'''GR-split of capital is coarser than data, so we must aggregate'''\n",
    "fixed_assets=fixed_assets.groupby(['k','i','year'],as_index=False).agg({'level':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ets.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ets=pd.read_excel(r'data\\ets.xlsx',keep_default_na=True)\n",
    "#reorder columns for free allowances and drop redundants\n",
    "qCO2_ETS_freeallowances=ets[['indu','year', 'free_allowances']]\n",
    "#level\n",
    "qCO2_ETS_freeallowances.rename(columns={'free_allowances':'level','indu':'i'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### emissions_brigde_items.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions_bridge_items=pd.read_excel(r'data\\emissions_brigde_items.xlsx',keep_default_na=True)\n",
    "\n",
    "qEmmLULUCF = emissions_bridge_items.loc[emissions_bridge_items['item'] == 'lulucf', ['year','co2_eq']]\n",
    "\n",
    "'''this is pretty fun.\n",
    "Year is stored as a floating point number 2020.0 - which is not the same as the string 2020.\n",
    "When exporting without converting to string, this then causes gamspy to look for an element corresponding to the floating point number 2020.0 in t, which it will not find.\n",
    "One can then ask: Why did this not happen when we loaded nEmployed?\n",
    "Because when we loaded nEmployed, we actually found it more convenient to construct a dataframe from scratch and populate with the members 2020 and some column sum from the data.\n",
    "'''\n",
    "qEmmLULUCF['year'] = qEmmLULUCF['year'].astype('string')\n",
    "\n",
    "qEmmLULUCF.rename(columns={'co2_eq':'level'},inplace=True)\n",
    "\n",
    "emissions_bridge_items_bordertrade=emissions_bridge_items.loc[emissions_bridge_items['item']=='bord_trade']\n",
    "emissions_bridge_items_bordertrade.rename(columns=dict_ebalitems,inplace=True)\n",
    "emissions_bridge_items_bordertrade=emissions_bridge_items_bordertrade.dropna(axis=1)\n",
    "#drop item\n",
    "emissions_bridge_items_bordertrade.drop(columns=['item'],inplace=True)\n",
    "#stack\n",
    "emissions_bridge_items_bordertrade.set_index('year',inplace=True)\n",
    "emissions_bridge_items_bordertrade=emissions_bridge_items_bordertrade.stack().to_frame(name='level').reset_index()\n",
    "#reorder columns\n",
    "emissions_bridge_items_bordertrade=emissions_bridge_items_bordertrade[['level_1','year','level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### government_finances.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "government_finances=pd.read_excel(r'data\\government_finances.xlsx',keep_default_na=True)\n",
    "\n",
    "'''Note:\n",
    "In GR, these values come from MAKRO.\n",
    "This makes it somewhat difficult to make sense of any deviations and/or constructed variables.\n",
    "Ask, if you find some bigguns\n",
    "'''\n",
    "#convert to string\n",
    "government_finances['year']=government_finances['year'].astype('string')\n",
    "#value2level\n",
    "government_finances.rename(columns={'value':'level'},inplace=True)\n",
    "#transfers to abroad\n",
    "government_finances_transfertorow=government_finances.loc[government_finances['trans']=='transfer_to_row']\n",
    "vGov2Foreign=government_finances_transfertorow[['year','level']]\n",
    "#transfers from abroaD\n",
    "government_finances_transferfromrow=government_finances.loc[government_finances['trans']=='transfers_from_row']\n",
    "vGovReceiveF=government_finances_transferfromrow[['year','level']]\n",
    "#Land rent\n",
    "government_finances_rent=government_finances.loc[government_finances['trans']=='rent']\n",
    "vGovRent=government_finances_rent[['year','level']]\n",
    "#government investments\n",
    "government_finances_invest=government_finances.loc[government_finances['trans']=='invest']\n",
    "vGovInv=government_finances_invest[['year','level']]\n",
    "#government subsidies\n",
    "government_finances_subsidies=government_finances.loc[government_finances['trans']=='subs']\n",
    "vGovSub=government_finances_subsidies[['year','level']]\n",
    "#capital transfers to domestic sectors\n",
    "government_finances_transferstofirms=government_finances.loc[government_finances['trans']=='cap_transfer_to_dom']\n",
    "vGov2Firms=government_finances_transferstofirms[['year','level']]\n",
    "#capital transfers from domestic firms\n",
    "government_finances_transfersfromfirms=government_finances.loc[government_finances['trans']=='cap_transfers_from_dom']\n",
    "vGovReceiveFirms=government_finances_transfersfromfirms[['year','level']]\n",
    "#Public expenditures, not including those paid by EU\n",
    "government_finances_exp = government_finances.loc[(government_finances['balance']=='exp') & (government_finances['trans']!='interest')]\n",
    "vGovExp=government_finances_exp[['year','level']]\n",
    "vGovExp=vGovExp.groupby(['year'],as_index=False).agg({'level':'sum'})\n",
    "#Public revenues (not including interests)\n",
    "government_finances_rev=government_finances.loc[(government_finances['balance']=='rev') &(government_finances['trans']!='interest')]\n",
    "vGovRev=government_finances_rev[['year','level']]\n",
    "vGovRev=vGovRev.groupby(['year'],as_index=False).agg({'level':'sum'})\n",
    "#Revenue from income taxation (kildeskatter)\n",
    "government_finances_source=government_finances.loc[government_finances['trans']=='tax_direct_source']\n",
    "vtSource=government_finances_source[['year','level']]\n",
    "#VAT\n",
    "government_finances_vat=government_finances.loc[government_finances['trans']=='tax_indirect_vat']\n",
    "vtVAT=government_finances_vat[['year','level']]\n",
    "#media tax¨\n",
    "government_finances_media=government_finances.loc[government_finances['trans']=='tax_direct_media']\n",
    "vtMedia=government_finances_media[['year','level']]\n",
    "#vehicles\n",
    "government_finances_vehicles=government_finances.loc[government_finances['trans']=='tax_direct_vehicles']\n",
    "vtCarWeight=government_finances_vehicles[['year','level']]\n",
    "#Revenue from indirect taxes (sum)\n",
    "government_finances_indirect=government_finances.loc[(government_finances['balance']=='rev') & (government_finances['trans'].str.contains('tax_indirect'))]\n",
    "vtIndirect=government_finances_indirect[['year','level']]\n",
    "vtIndirect=vtIndirect.groupby(['year'],as_index=False).agg({'level':'sum'})\n",
    "#Revenue from direct taxes (sum)\n",
    "government_finances_direct=government_finances.loc[(government_finances['trans'].str.contains('tax_direct'))&(government_finances['balance']=='rev')]\n",
    "vtDirect=government_finances_direct[['year','level']]\n",
    "vtDirect=vtDirect.groupby(['year'],as_index=False).agg({'level':'sum'})\n",
    "#Rest\n",
    "government_finances_other=government_finances.loc[(government_finances['balance']=='rev')&(~government_finances['trans'].str.contains('tax_direct'))&(~government_finances['trans'].str.contains('tax_indirect'))]\n",
    "vGovRevRest=government_finances_other[['year','level']]\n",
    "vGovRevRest=vGovRevRest.groupby(['year'],as_index=False).agg({'level':'sum'})\n",
    "#final public consumption\n",
    "government_finances_final=government_finances.loc[government_finances['trans']=='cons_publ']\n",
    "vG=government_finances_final[['year','level']]\n",
    "#gov transfers\n",
    "government_finances_transfer=government_finances.loc[government_finances['trans']=='transfer_to_hh']\n",
    "vTrans=government_finances_transfer[['year','level']]\n",
    "# #social contributions\n",
    "government_finances_social=government_finances.loc[government_finances['trans']=='soc_cont']\n",
    "vCont=government_finances_social[['year','level']]\n",
    "#revenue from corporate taxation\n",
    "government_finances_corp=government_finances.loc[government_finances['trans']=='tax_direct_corp']\n",
    "vtCorp=government_finances_corp[['year','level']]\n",
    "#tax on pension\n",
    "government_finances_pension=government_finances.loc[government_finances['trans']=='tax_direct_pension']\n",
    "vtPAL=government_finances_pension[['year','level']]\n",
    "\n",
    "'''To split taxes on personal income, we use the disaggregated sheet from the same xlsx-file.\n",
    "Obviously one can just look at it and recognize the numbers in the aggregated sheet and load directly therefrom, but since\n",
    "they are indistinguishable - in all other ways than the value in the aggregated form, I read from the diaggregated sheet for inspectability.\n",
    "Here I am forced to use the trans_txt-column to distinguish between the personal income taxes since they are otherwise identically labelled\n",
    "'''\n",
    "#tax on labour\n",
    "government_finances_disagg=pd.read_excel(r'data\\government_finances.xlsx',sheet_name='gov_fin_disagg',keep_default_na=True)\n",
    "government_finances_disagg.rename(columns={'value':'level'},inplace=True)\n",
    "government_finances_disagg['year']=government_finances_disagg['year'].astype('string')\n",
    "#revenue from contribution to labour market fund\n",
    "government_finances_taxlaborAM=government_finances_disagg.loc[government_finances_disagg['trans_txt'].str.contains('labour market fund')&(government_finances_disagg['trans']=='tax_direct_other_labor')]\n",
    "vtAM=government_finances_taxlaborAM[['year','level']]\n",
    "#other personal income taxes\n",
    "government_finances_taxlaboroth=government_finances_disagg.loc[(government_finances_disagg['trans']=='tax_direct_other_labor')&(government_finances_disagg['trans_txt'].str.contains('other'))]\n",
    "vtPersIncRest=government_finances_taxlaboroth[['year','level']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### institutional_financial_accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>li</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020</td>\n",
       "      <td>12.058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year      li\n",
       "6  2020  12.058"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "institutional_financial_accounts=pd.read_excel(r'data\\institutional_financial_accounts.xlsx',keep_default_na=True)\n",
    "#make sure year is string\n",
    "institutional_financial_accounts['year']=institutional_financial_accounts['year'].astype('string')\n",
    "\n",
    "#gov interests\n",
    "vGovInterest=institutional_financial_accounts.loc[(institutional_financial_accounts['var']=='vNetInterests')&(institutional_financial_accounts['sector']=='gov')]\n",
    "#net\n",
    "vGovNetInterest=vGovInterest[['year','net']]\n",
    "vGovNetInterest.rename(columns={'net':'level'})\n",
    "#assets\n",
    "vInterestGovAssets=vGovInterest[['year','as']]\n",
    "vInterestGovAssets.rename(columns={'as':'level'})\n",
    "#debt\n",
    "vInterestGovDebt=vGovInterest[['year','li']]\n",
    "vInterestGovDebt.rename(columns={'as':'level'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading metadata for sets + set_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate c and add text\n",
    "metadata_cons_hh=pd.read_excel(r'data\\metadata.xlsx',sheet_name='cons_hh',keep_default_na=True)\n",
    "c_records = list(metadata_cons_hh.itertuples(index=False, name=None))\n",
    "#populate i and re (with text)\n",
    "metadata_industries=pd.read_excel(r'data\\metadata.xlsx',sheet_name='industries',keep_default_na=True)\n",
    "i_records = list(metadata_industries.itertuples(index=False, name=None))\n",
    "i_records_fortot=i_records.copy()\n",
    "re_records = [(str(x) + '_re', y) for x, y in i_records]\n",
    "#populate es (w. text)\n",
    "metadata_energy_purposes=pd.read_excel(r'data\\metadata.xlsx',sheet_name='energy_purposes',keep_default_na=True)\n",
    "es_records = list(metadata_energy_purposes.itertuples(index=False, name=None))\n",
    "#ppulate a_rows_ (w. text)\n",
    "metadata_flows=pd.read_excel(r'data\\metadata.xlsx',sheet_name='flows',keep_default_na=True)\n",
    "metadata_flows_a=metadata_flows[metadata_flows['flow_type']=='prim_input']\n",
    "metadata_flows_a['flow']=metadata_flows_a['flow'].replace(dict_a)\n",
    "a_records=list(metadata_flows_a[['flow','flow_txt']].itertuples(index=False, name=None))\n",
    "#populate k (w. text)\n",
    "metadata_flows_k=metadata_flows[metadata_flows['flow'].str.contains('invest')]\n",
    "metadata_flows_k['flow']=metadata_flows_k['flow'].replace(io_inv_dict)\n",
    "k_records=list(metadata_flows_k[['flow','flow_txt']].itertuples(index=False, name=None))\n",
    "k_records_fortot=k_records.copy()\n",
    "'''\n",
    "populate ebalitems, currently 'EAFG_tax is called explicitly in the model and is not present in data, so I add it manually to the set ebalitems and the subset etaxes.\n",
    "'''\n",
    "ebalitems_records=list(set(non_energy_emissions['ebalitems']).union(set(energy_and_emissions['ebalitems'])))\n",
    "ebalitems_records.append('EAFG_tax')\n",
    "#etaxes records\n",
    "etaxes_records=[s for s in ebalitems_records if '_tax' in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAMS export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sets'''\n",
    "transaction=gp.Set(m,name='transaction',description='set of transaction types')\n",
    "es=gp.Set(m,'es',description='energy service',records=es_records)\n",
    "out=gp.Set(m,'out',description='output types',records=out_vals)\n",
    "e=gp.Set(m,'e',domain=[out],description='energy products by industry',records=e_vals)\n",
    "t=gp.Set(m,'t',description='year',records=t_list)\n",
    "t1=gp.Set(m,'t1',domain=[t],description='t1',is_singleton=True,records=['2020'])\n",
    "a_rows_=gp.Set(m,'a_rows_',description='other rows in the input-output table',records=a_records)\n",
    "k=gp.Set(m,name=\"k\",description='capital types',records=k_records)\n",
    "'''ebalitems + subsets, some are manually populated since they lack a sufficiently universal identifier in the data'''\n",
    "ebalitems=gp.Set(m,'ebalitems',description='identifiers tax joules prices etc for energy components by demand components',records=ebalitems_records)\n",
    "em=gp.Set(m,name='em',domain=[ebalitems],description='emission types',records=['ch4','co2ubio','n2o','co2e','co2bio'])\n",
    "etaxes=gp.Set(m,name='etaxes',domain=[ebalitems],description='taxes from ebalitems',records=etaxes_records)\n",
    "'''d + subsets of d'''\n",
    "d=gp.Set(m,'d',description='demand components',records=list(set(io_combined_y['d']).union(set(io_combined_m['d']),set(io_y['d']),set(io_m['d']),set(io_combined_a['d']),set(io_a['d']),set(non_energy_emissions['d']),set(energy_and_emissions['d']),set(i_re_elements))))\n",
    "invt=gp.Set(m,name='invt',domain=[d],description='Inventories',is_singleton=True,records=['invt'])\n",
    "i=gp.Set(m,name='i',domain=[d],description='sectors',records=i_records)\n",
    "tl=gp.Set(m,name='tl',domain=[d],description='Transmission losses',is_singleton=True ,records=['tl'])\n",
    "x=gp.Set(m,name='x',domain=[d],description='export types',records=['xOth'])\n",
    "g=gp.Set(m,name='g',domain=[d],description='public consumption',records=['g'])\n",
    "c=gp.Set(m,name='c',domain=[d],description='private consumption groups',records=c_records)\n",
    "rx=gp.Set(m,name='rx',domain=[d],description='Non-energy intermediate input types, this is just equal to i ATM.',records=i_records)\n",
    "re=gp.Set(m,name='re',domain=[d],description='Energy intermediate input types',records=re_records)\n",
    "rx2re=gp.Set(m,name='rx2re',domain=[rx,re],description='map from rx to re',records=rx2re_list)\n",
    "'''non-energy emissions'''\n",
    "non_energy_emissions=gp.Parameter(m,name='NonEnergyEmissions',domain=[ebalitems,transaction,d,t],description='emission from consumption of non-energy',records=non_energy_emissions.values.tolist(),domain_forwarding=True)\n",
    "\n",
    "'''energy emissions'''\n",
    "EnergyBalance=gp.Parameter(m,'EnergyBalance',domain=[ebalitems,transaction,d,es,out,t],description='Main data input with regards to energy and energy-related emissions',records=energy_and_emissions[['ebalitems','transaction','d','es','e','year','level']].values.tolist(),domain_forwarding=True)\n",
    "'''demand_transaction ⊂ transaction, transaction is currently populated using domain_forwarding, meaning it is not populated before EnergyBalance and NonEnergyEmissions are defined'''\n",
    "demand_transaction=gp.Set(m,name='demand_transaction',domain=[transaction],description='Demand components',records=['production','input_in_production','export','inventory','transmission_losses'])\n",
    "'''IO'''\n",
    "vIO_y=gp.Parameter(m,name='vIO_y',domain=[d,d,t],description='Production IO',records=io_combined_y[['i', 'd', 't', 'level']].values.tolist(),domain_forwarding=True)\n",
    "vIO_m=gp.Parameter(m,name='vIO_m',domain=[d,d,t],description='Production IO',records=io_combined_m[['i', 'd', 't', 'level']].values.tolist(),domain_forwarding=True)\n",
    "\n",
    "\n",
    "vIOxE_y=gp.Parameter(m,name='vIOxE_y',domain=[d,d,t],description='non-energy IO of domestic production',records=io_y[['i', 'd', 't', 'level']].values.tolist(),domain_forwarding=True)\n",
    "vIOxE_m=gp.Parameter(m,name='vIOxE_m',domain=[d,d,t],description='non-energy IO of imports',records=io_m[['i', 'd', 't', 'level']].values.tolist(),domain_forwarding=True)\n",
    "\n",
    "vIO_a=gp.Parameter(m,name='vIO_a',domain=[a_rows_,d,t],description='other IO',records=io_combined_a[['i', 'd', 't', 'level']].values.tolist(),domain_forwarding=True)\n",
    "vIOxE_a=gp.Parameter(m,name='vIOxE_a',domain=[a_rows_,d,t],description='non energy other IO',records=io_a[['i', 'd', 't', 'level']].values.tolist(),domain_forwarding=True)\n",
    "\n",
    "'''demand side IO'''\n",
    "qI_k_i=gp.Parameter(m,'qI_k_i',domain=[k,d,t],description='Real capital stock by capital type and industry',records=io_inv_qI_k_i_agg[['k','i','year','level']].values.tolist())\n",
    "wagesum_with_t=gp.Parameter(m,name='qL',domain=[d,t],description='Wage expenses',records=wagesum_with_t[['i','t','level']].values.tolist())\n",
    "nemployed=gp.Parameter(m,name='nEmployed',domain=[t],description='Total number of employees including independents',records=nemployed_frame.values.tolist())\n",
    "\n",
    "'''capital fixed assets'''\n",
    "fixed_assets=gp.Parameter(m,name='qK',domain=[k,d,t],description='Capital split on types and sectors',records=fixed_assets[['k','i','year','level']].values.tolist())\n",
    "\n",
    "'''ets'''\n",
    "qCO2_ETS_freeallowances=gp.Parameter(m,name='qCO2_ETS_freeallowances',domain=[d,t],description='CO2-ETS free allowances',records=qCO2_ETS_freeallowances[['i','year','level']].values.tolist())\n",
    "\n",
    "'''emissions bridge items'''\n",
    "qEmmLULUCF=gp.Parameter(m,name='qEmmLULUCF',domain=[t],description='Total LULUCF-emissions',records=qEmmLULUCF.values.tolist())\n",
    "qEmmBorderTrade=gp.Parameter(m,name='qEmmBorderTrade',domain=[em,t],description='emissions from border trade',records=emissions_bridge_items_bordertrade.values.tolist())\n",
    "\n",
    "'''government finances'''\n",
    "vGov2Foreign=gp.Parameter(m,name='vGov2Foreign',domain=[t],description='Payments to foreign countries',records=vGov2Foreign.values.tolist())\n",
    "vGovReceiveF=gp.Parameter(m,name='vGovReceiveF',domain=[t],description='Payments from foreign contries',records=vGovReceiveF.values.tolist())\n",
    "vGovRent=gp.Parameter(m,name='vGovRent',domain=[t],description='Land rent',records=vGovRent.values.tolist())\n",
    "vGovInv=gp.Parameter(m,name='vGovInv',domain=[t],description='Government Investments',records=vGovInv.values.tolist())\n",
    "vGovSub=gp.Parameter(m,name='vGovSub',domain=[t],description='Government subsidies',records=vGovSub.values.tolist())\n",
    "vGov2Firms=gp.Parameter(m,name='vGov2Firms',domain=[t],description='payments to domestic firms',records=vGov2Firms.values.tolist())\n",
    "vGovReceiveFirms=gp.Parameter(m,name='vGovReceiveFirms',domain=[t],description='Payments from domestic firms',records=vGovReceiveFirms.values.tolist())\n",
    "vGovExp=gp.Parameter(m,name='vGovExp',domain=[t],description='Government expenditures except interest payments',records=vGovExp.values.tolist())\n",
    "vGovRev=gp.Parameter(m,name='vGovRev',domain=[t],description='Publiv revenue except interest payments',records=vGovRev.values.tolist())\n",
    "vtSource=gp.Parameter(m,name='vtSource',domain=[t],description='Revenue from income taxation (kildeskatter)',records=vtSource.values.tolist())\n",
    "vtVAT=gp.Parameter(m,name='vtVAT',domain=[t],description='Total revenue from VAT',records=vtVAT.values.tolist())\n",
    "vtMedia=gp.Parameter(m,name='vtMedia',domain=[t],description='Revenue from public media contribution',records=vtMedia.values.tolist())\n",
    "vtCarWeight=gp.Parameter(m,name='vtCarWeight',domain=[t],description='Revenue from taxation  on paid weight charge',records=vtCarWeight.values.tolist())\n",
    "vtIndirect=gp.Parameter(m,name='vtIndirect',domain=[t],description='Revenue from indirect taxes',records=vtIndirect.values.tolist())\n",
    "vtDirect=gp.Parameter(m,name='vtDirect',domain=[t],description='Revenue from direct taxes',records=vtDirect.values.tolist())\n",
    "vGovRevRest=gp.Parameter(m,name='vGovRevRest',domain=[t],description='Other government revenues',records=vGovRevRest.values.tolist())\n",
    "vG=gp.Parameter(m,name='vG',domain=[t],description='Value of public consumption',records=vG.values.tolist())\n",
    "vTrans=gp.Parameter(m,name='vTrans',domain=[t],description='Government transfer payments',records=vTrans.values.tolist())\n",
    "vCont=gp.Parameter(m,name='vCont',domain=[t],description='Contributions (bidrag til sociale ordninger)',records=vCont.values.tolist())\n",
    "vtCorp=gp.Parameter(m,name='vtCorp',domain=[t],description='Revenue from corporate taxation',records=vtCorp.values.tolist())\n",
    "vtPAL=gp.Parameter(m,name='vtPAL',domain=[t],description='PAL tax revenue',records=vtPAL.values.tolist())\n",
    "vtAM=gp.Parameter(m,name='vtAM',domain=[t],description='Revenue on taxation from payroll to the labour market institutions',records=vtAM.values.tolist())\n",
    "vtPersIncRest=gp.Parameter(m,name='vtPersIncRest',domain=[t],description='Revenue from taxation on other personal income',records=vtPersIncRest.values.tolist())\n",
    "\n",
    "'''institutional financial accounts'''\n",
    "vGovNetInterest=gp.Parameter(m,name='vGovNetInterest',domain=[t],description='The government net interests',records=vGovNetInterest.values.tolist())\n",
    "vInterestGovAssets=gp.Parameter(m,name='vInterestGovAssets',domain=[t],description='Interest payments on governmnet assets',records=vInterestGovAssets.values.tolist())\n",
    "vInterestGovDebt=gp.Parameter(m,name='vInterestGovDebt',domain=[t],description='Interest payments on government liabilities',records=vInterestGovDebt.values.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hardcoded + export gdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HARD CODED SETS'''\n",
    "factors_of_production=gp.Set(m,name='factors_of_production',description='factors of production, hardcoded',records=['iM','iB','iT','labor','RxE','machine_energy','transport_energy','heating_energy','refinery_crudeoil','naturalgas_for_distribution','biogas_for_processing'])\n",
    "em_accounts=gp.Set(m,name='em_accounts',description='Different accounting levels of emissions inventories',records=['GNA','UNFCCC','GNA_lulucf','UNFCCC_lulucf'])\n",
    "land5=gp.Set(m,name='land5',records=['forest','wetland','grassland','crop','settlement'])\n",
    "'''sets already made that need tots'''\n",
    "i_records_fortot.append(('tot','total'))\n",
    "k_records_fortot.append(('iTot','total'))\n",
    "i_=gp.Set(m,name='i_',description='sectors, including total',records=i_records_fortot)\n",
    "k_=gp.Set(m,name='k_',description='capital types including total, excluding inventories',records=k_records_fortot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additions required for base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''13.3.25:\n",
    "In order for the base-model to run using output from the data processing script, some extra objects whose origin is not explicitly clear in the current data package.\n",
    "It is also not certain that these objects are part of the long-term plan.\n",
    "In order to make the model run, I will create these objects here.\n",
    "'''\n",
    "\n",
    "'''Object 1 is the set m, which is a subset of i containing \"industries with imports\". \n",
    "This can be interpreted as either industries abroad that produce stuff that we import (corresponding to the rows in the import-section of the IO-table),\n",
    "or industries that import something from abroad corresponding to the columns.\n",
    "In our case this does not really matter if we consider energy-products as well as ordinary sector-specific outputs.\n",
    "If energy is not counted, the former of the two interpretations does indeed give rise to a proper subset. I, however will for the time being consider energy as well\n",
    "'''\n",
    "\n",
    "m_=gp.Set(m,name='m',domain=[i],description='industries with imports',records=io_combined_m['i'].unique().tolist())\n",
    "\n",
    "m.write('dataa_ny.gdx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
