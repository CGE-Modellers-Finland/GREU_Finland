{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import gams.core.numpy\n",
    "import gamspy as gp\n",
    "from global_container import container, Set, Variable, Parameter\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're going to read data to a .gdx file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAMSPy set-up + introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUPDATE THIS:\\nThe data we read in this file is from several different .xlsx sheets. I always read all the data from one spreadsheet before starting another, and this is always indicated by a header named filename.xlsx.\\nThere may be several chunks of code dedicated to a single spreadsheet depending on the side. Unless there is a header named filename.xlsx, the chunk of code treats data from the same spreadsheet as the previous chunk.\\nThe packages I use are all imported in the chunk immediately above this one. Longer comments, or comments that I think contain information that may be relevant to the reader is commented out like here, whereas\\ncomments that are mainly for the sake of structure and do not contain new or noteworthy information are just hashtagged out.\\nAs of 20/2/25 this script is unrevised by others. \\nThe script runs reasonably efficiently with a few exceptions, it does however rely quite heavily on the pandas-package atm.\\nIn terms of robustness, I rely quite heavily on the naming conventions and datastructure in the datasheets.\\nThe names are not extracted, but typed which comes with some risk should those conventions change.\\nThe gp.Set(), gp.Parameter()-functions that I rely on for datatransfer are also snesitive to changes in the data-structure, that is domain-names must be entered in the \"correct\" order, especially when relying on the\\nvery efficient domain_forwarding for inhabiting sets.\\nSets and variables are named deliberately to be as unequivocal as possible regarding what input-data from the spreadsheets they contain. \\nThis means that many objects have rather long, tongue-twisting names. The general naming convention is spreadsheet_somesubset_somesubsetofsubset such that ideally, the output generated at the end of this script,\\ncan be traced back to a cell(s) in the spreadsheet by anyone pedantic enough to want to do so without having to inspect the code.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=gp.Container()\n",
    "#product=gp.Set(m,name=\"product\",description=\"energy product type, e.g. coal, electricity, etc.\")\n",
    "#purpose=gp.Set(m,name=\"purpose\",description=\"Purpose of energy usage, e.g. heating, transport, etc.\")\n",
    "#indu=gp.Set(m,name=\"indu\",description=\"Industry, energy sectors,c and tax. Tax is from io, maybe split idk\")\n",
    "#tax=gp.Set(m,name=\"tax\",description=\"tax type, e.g. CO2 tax, etc.\")\n",
    "#item=gp.Set(m,name=\"item\",description=\"emissions differing by accounting method\")\n",
    "\n",
    "'''\n",
    "UPDATE THIS: Fix this intro\n",
    "\n",
    "\n",
    "The data we read in this file is from several different .xlsx sheets. I always read all the data from one spreadsheet before starting another, and this is always indicated by a header named filename.xlsx.\n",
    "There may be several chunks of code dedicated to a single spreadsheet depending on the side. Unless there is a header named filename.xlsx, the chunk of code treats data from the same spreadsheet as the previous chunk.\n",
    "The packages I use are all imported in the chunk immediately above this one. Longer comments, or comments that I think contain information that may be relevant to the reader is commented out like here, whereas\n",
    "comments that are mainly for the sake of structure and do not contain new or noteworthy information are just hashtagged out.\n",
    "As of 20/2/25 this script is unrevised by others. \n",
    "The script runs reasonably efficiently with a few exceptions, it does however rely quite heavily on the pandas-package atm.\n",
    "In terms of robustness, I rely quite heavily on the naming conventions and datastructure in the datasheets.\n",
    "The names are not extracted, but typed which comes with some risk should those conventions change.\n",
    "The gp.Set(), gp.Parameter()-functions that I rely on for datatransfer are also snesitive to changes in the data-structure, that is domain-names must be entered in the \"correct\" order, especially when relying on the\n",
    "very efficient domain_forwarding for inhabiting sets.\n",
    "Sets and variables are named deliberately to be as unequivocal as possible regarding what input-data from the spreadsheets they contain. \n",
    "This means that many objects have rather long, tongue-twisting names. The general naming convention is spreadsheet_somesubset_somesubsetofsubset such that ideally, the output generated at the end of this script,\n",
    "can be traced back to a cell(s) in the spreadsheet by anyone pedantic enough to want to do so without having to inspect the code.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dictionaries for mapping from raw data onto GR-set members'''\n",
    "\n",
    "dict_e={'liq_biofuel':'Biooil','other_oil':'Oil products','coal':'Coal and coke','firewood':'Firewood and woodchips','refin_gas':'Refinery gas','semi_refin_oil':'Semi-refined oil','straw':'Straw for energy purposes','crude':'Crude oil','bunk_planes':'Bunkering of Danish operated planes on foreign territory','bunk_ships':'Bunkering of Danish operated vessels on foreign territory','bunk_trucks':'Bunkering of Danish operated trucks on foreign territory','jet_petro':'Jet petroleum','diesel_transp':'Diesel for transport','gasol_transp':'Gasoline for transport','heat_pump':'Heat pumps','renewable':'Renewable energy','waste_oil':'Waste oil','wood_waste':'Wood waste','wood_pellets':'Wood pellets','district_heat':'District heat','natgas':'Natural gas incl. biongas','natgas_extract':'Natural gas (Extraction)'}\n",
    "\n",
    "dict_transaction={'transmis_loss':'transmission_losses','cons_inter':'input_in_production','cons_hh':'household_consumption','import':'imports','invent_change':'inventory'}\n",
    "\n",
    "dict_ebalitems={'ws_marg':'EAV','ret_marg':'DAV','basic':'BASE','co2_xbio':'co2ubio','co2_eq':'co2e','co2_bio':'co2bio'}\n",
    "\n",
    "dict_a={'tax_products':'TaxSub','tax_vat':'Moms','emp_comp':'SalEmpl','subs_other_production':'OthSubs','tax_other_production':'OthTax','gross_surplus':'OvProd'} #turister"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### energy_and_emissions.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\830845114.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  energy_and_emissions['es'].fillna('unspecified',inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\830845114.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  energy_and_emissions['transaction'].replace(dict_transaction,inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\830845114.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  energy_and_emissions['ebalitems'].replace(dict_ebalitems,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "energy_and_emissions=pd.read_excel(r'data\\energy_and_emissions.xlsx',keep_default_na=True)\n",
    "\n",
    "'''rename coslumns for compatibility with data loading'''\n",
    "energy_and_emissions.rename(columns={'indu':'d','product':'e','purp':'es','flow':'transaction'},inplace=True)\n",
    "energy_and_emissions.set_index(['year','bal','transaction','d','es','e'],drop=True,inplace=True)\n",
    "\n",
    "energy_and_emissions=energy_and_emissions.stack().to_frame(name='level')\n",
    "energy_and_emissions.index.set_names(['year','bal','transaction','d','es','e','ebalitems'],inplace=True)\n",
    "energy_and_emissions.reset_index(inplace=True)\n",
    "energy_and_emissions['es'].fillna('unspecified',inplace=True)\n",
    "\n",
    "energy_and_emissions['transaction'].replace(dict_transaction,inplace=True)\n",
    "energy_and_emissions['ebalitems'].replace(dict_ebalitems,inplace=True)\n",
    "\n",
    "'''replace those pesky nans with a target-specific label'''\n",
    "energy_and_emissions['d'] = energy_and_emissions.apply(lambda row: 'xOth' if pd.isna(row['d']) and row['transaction'] == 'export' \n",
    "                               else ('invt' if pd.isna(row['d']) and row['transaction'] == 'inventory' \n",
    "                                     else ('tl' if pd.isna(row['d']) and row['transaction'] == 'transmission_losses' \n",
    "                                           else ('natural_input' if pd.isna(row['d']) and row['transaction'] == 'nat_input'\n",
    "                                                 else ('residual' if pd.isna(row['d']) and row['transaction'] == 'res_input'\n",
    "                                                       else ('19000' if pd.isna(row['d']) and row['transaction']=='imports'\n",
    "                                                            else row['d']))))), axis=1)\n",
    "\n",
    "energy_and_emissions['e']=energy_and_emissions['e'].replace({'liq_biofuel':'Biooil','other_oil':'Oil products','coal':'Coal and coke','firewood':'Firewood and woodchips','refin_gas':'Refinery gas','semi_refin_oil':'Semi-refined oil','straw':'Straw for energy purposes','crude':'Crude oil','bunk_planes':'Bunkering of Danish operated planes on foreign territory','bunk_ships':'Bunkering of Danish operated vessels on foreign territory','bunk_trucks':'Bunkering of Danish operated trucks on foreign territory','jet_petro':'Jet petroleum','diesel_transp':'Diesel for transport','gasol_transp':'Gasoline for transport','heat_pump':'Heat pumps','renewable':'Renewable energy','waste_oil':'Waste oil','wood_waste':'Wood waste','wood_pellets':'Wood pellets','district_heat':'District heat','natgas':'Natural gas incl. biongas','natgas_extract':'Natural gas (Extraction)'})\n",
    "'''Bioethanol og biodiesel mangler'''\n",
    "\n",
    "'''add t=2019'''\n",
    "#energy_and_emissions=pd.concat([energy_and_emissions,pd.DataFrame(columns=['t'],index=energy_and_emissions.index,data=2020)],axis=1)\n",
    "\n",
    "\n",
    "# set column order\n",
    "energy_and_emissions = energy_and_emissions[['ebalitems', 'transaction', 'd', 'es', 'e', 'year', 'level']]\n",
    "\n",
    "'''build sets'''\n",
    "transaction=gp.Set(m,name='transaction',description='HEY. set of transaction types')\n",
    "'''We add 'energy' manually to d, because it does not appear in data,\n",
    "and in order to define equations in the model on the demand component 'energy', we need to first declare that it\n",
    "is in fact a set member. '''\n",
    "#d=gp.Set(m,'d',description='demand components',records='energy')\n",
    "d=gp.Set(m,'d',description='HEY. demand components')\n",
    "es=gp.Set(m,'es',description='HEY.energy service')\n",
    "e=gp.Set(m,'e',description='HEY. energy products by industry')\n",
    "ebalitems=gp.Set(m,'ebalitems',description='HEY. identifiers tax joules prices etc for energy components by demand components')\n",
    "t=gp.Set(m,'t',description='HEY. year')\n",
    "\n",
    "EnergyBalance=gp.Parameter(m,'EnergyBalance',domain=[ebalitems,transaction,d,es,e,t],description='Main data input with regards to energy and energy-related emissions',records=energy_and_emissions.values.tolist(),domain_forwarding=True)\n",
    "\n",
    "demand_transaction=gp.Set(m,name='demand_transaction',domain=[transaction],description='hej',records=['production','input_in_production','export','inventory','transmission_losses'])\n",
    "em=gp.Set(m,name='em',domain=[ebalitems],description='HEY. emission types',records=['ch4','co2ubio','n2o','co2e','co2bio'])\n",
    "etaxes=gp.Set(m,name='etaxes',domain=[ebalitems],description='HEY. taxes from ebalitems',records=['co2_tax','so2_tax','nox_tax','ener_tax','pso_tax'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonenergyemissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2773027590.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  non_energy_emissions['transaction'].replace(dict_transaction,inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2773027590.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  non_energy_emissions['ebalitems'].replace(dict_ebalitems,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "non_energy_emissions=pd.read_excel(r'data\\non_energy_emissions.xlsx',keep_default_na=True)\n",
    "non_energy_emissions.set_index(['bal','flow','indu'],inplace=True)\n",
    "non_energy_emissions.drop(columns=['year'],inplace=True) #annoying manual bs, will fix later\n",
    "non_energy_emissions=non_energy_emissions.stack().to_frame(name='level')\n",
    "non_energy_emissions.dropna(inplace=True)\n",
    "non_energy_emissions.index.rename(['bal','transaction','d','ebalitems'],inplace=True)\n",
    "non_energy_emissions.reset_index(inplace=True)\n",
    "non_energy_emissions.drop(columns='bal',inplace=True)\n",
    "\n",
    "non_energy_emissions['transaction'].replace(dict_transaction,inplace=True)\n",
    "non_energy_emissions['ebalitems'].replace(dict_ebalitems,inplace=True)\n",
    "'''\n",
    "t-column\n",
    "'''\n",
    "non_energy_emissions_t=pd.concat([non_energy_emissions,pd.DataFrame(columns=['t'],index=non_energy_emissions.index,data=2020)],axis=1)\n",
    "\n",
    "\n",
    "# set column order\n",
    "non_energy_emissions_t = non_energy_emissions_t[['ebalitems', 'transaction', 'd', 't', 'level']]\n",
    "\n",
    "non_energy_emissions_t=gp.Parameter(m,name='NonEnergyEmissions',domain=[ebalitems,transaction,d,t],description='HEY. emission from consumption of non-energy',records=non_energy_emissions_t.values.tolist(),domain_forwarding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IO-loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_y.drop(columns=['row_l1'],inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:89: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_m.drop(columns=['row_l1'],inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:90: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_ene_a.drop(columns=['row_l1'],inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:91: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_ene_y.drop(columns=['row_l1'],inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_ene_m.drop(columns=['row_l1'],inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:93: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_a.drop(columns=['row_l1'],inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:95: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  io_a['i'].replace(dict_a,inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_a['i'].replace(dict_a,inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:96: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  io_ene_a['i'].replace(dict_a,inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\2154421953.py:96: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_ene_a['i'].replace(dict_a,inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Note: There are many sectors here that are not represented in the other IO-tables, should be brought up to date'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''gampy does not support domain forwarding from set to set.\n",
    "To construct a superset of IO-sectors we must initially populate the superset, then provide it as domain for subsets, which we can then populate using the tried and tested domain_forwarding method.\n",
    "To populate the superset, we must load data from the io-spreadsheets.\n",
    "On the data:\n",
    "The Danish IO-tables are received in long-format and matrix-format. I read the long-format for convenience, but the content of the data is better understood when veiwing in matrix-format.\n",
    "The columns consist of demand-components, which can be either sectors of the economy, export, investment or some final-demand component such as food or housing.\n",
    "The rows consist of inputs, and can be subdivided into three categories, domestic production, import and primary inputs.\n",
    "The primary inputs are taxes, subsidies, employee compensation and the likes.\n",
    "Imported and domestically produced inputs are industry-outputs such that a number in any entry of the IO-matrix can be read as the supply from the row-index to the column-index.\n",
    "To make this data compatible with the model, we must first inform the model how to interpret the indices. We do this by defining sets.\n",
    "For instance the set \"i\" is the set of sectors in the model, we can therefore say that the rows of the import- and domestically produced part of the io-table, consist of members of i. \n",
    "The set d is the set of demand-components and corresponds to the columns of the io-tables.\n",
    "By creating separate variables for imported and domestically produced supply from the io-table, we can define a variable like vIO_y, and define in on [i,d,t], which we can then read as \n",
    "domestic sector i's supply to demand component d at year t. Atm, I think the easiest to follow in the extremely likely event that data is not in the exact same format, is the current version\n",
    "in which I explicitly label columns in the dataframes according to their GR-set-counterparts. \n",
    "It is important not to mess with the sets, as they are called explicitly in the model, always by name and on occasion we also refer to specific elements.\n",
    "The consequence of this is that names have to be inserted here. This will almost certainly need to be edited for other sources of data.\n",
    "I try to make apparent from the naming throughout where columns and rows are meant to end up in the model which\n",
    "I suspect will ultimately be more convenient for adapting this code to other datasets than attempting to accomplish the\n",
    "highest possible degree of automization.\n",
    "'''\n",
    "#energy\n",
    "io_energy=pd.read_excel(r'data\\io_energy_long_format.xlsx',keep_default_na=True)\n",
    "io_energy_forlater=io_energy.copy(deep=True)\n",
    "io_energy.rename(columns={'row_l2':'i','col_l1':'DELETE','col_l2':'d','value':'level'},inplace=True)\n",
    "\n",
    "#regular\n",
    "io=pd.read_excel(r'data\\io_long_format.xlsx')\n",
    "io_forlater=io.copy(deep=True)\n",
    "io.rename(columns={'row_l2':'i','col_l1':'DELETE','col_l2':'d','value':'level'},inplace=True)\n",
    "\n",
    "'''fill nans based on previous entries. In Danish dataset, col_l1 contains supercategories for col_l2.\n",
    "Meanwhile, we use col_l2, which contains useful categories such as sectors (as opposed to cons_inter). \n",
    "Categories which do not have a subcategory, such as cons_publ (public consumption) have an empty cell at col_l2.\n",
    "In this program I create the set of demand components d from its members, so in order to represent the demand components that do\n",
    "not have multiple subcategories, I run the lines below which fills the appropriate GR-demand component based on the supercategory, i.e.\n",
    "if row10 supercategory is export, replace NaN in the demand component column of row10 with xOth.\n",
    "'''\n",
    "io['d'] = io.apply(lambda row: 'xOth' if pd.isna(row['d']) and row['DELETE'] == 'export' \n",
    "                               else ('invt' if pd.isna(row['d']) and row['DELETE'] == 'invent_change' \n",
    "                                     else ('g' if pd.isna(row['d']) and row['DELETE'] == 'cons_publ' \n",
    "                                           else ('iB' if pd.isna(row['d']) and row['DELETE'] == 'invest_build'\n",
    "                                                 else ('iT' if pd.isna(row['d']) and row['DELETE'] == 'invest_trans'\n",
    "                                                       else ('iM' if pd.isna(row['d']) and row['DELETE']=='invest_other'\n",
    "                                                            else row['d']))))), axis=1)\n",
    "\n",
    "io_energy['d'] = io_energy.apply(lambda row: 'xOth' if pd.isna(row['d']) and row['DELETE'] == 'export' \n",
    "                               else ('invt' if pd.isna(row['d']) and row['DELETE'] == 'invent_change' \n",
    "                                     else ('g' if pd.isna(row['d']) and row['DELETE'] == 'cons_publ' \n",
    "                                           else ('iB' if pd.isna(row['d']) and row['DELETE'] == 'invest_build'\n",
    "                                                 else ('iT' if pd.isna(row['d']) and row['DELETE'] == 'invest_trans'\n",
    "                                                       else ('iM' if pd.isna(row['d']) and row['DELETE']=='invest_other'\n",
    "                                                            else row['d']))))), axis=1)\n",
    "\n",
    "'''Editors note: At the time of writing, the model is not equipped to handle the distinction between con_hh and cons_hh_foreign, therefore\n",
    "for the time being we simply add them together, I do this by creating a boolean mask to identify rows where the aforementioned supercategory-column \n",
    "contains the string cons_hh, which in the GR dataset is both cons_hh and cons_hh_foreign_tou. I then build a dataframe of the entries with only the members of\n",
    "the original frame(s) that has has cons_hh or cons_hh_foreign_tou as supercategory. I then group entries in this dataset based on entries in the other columns (except of course level) and add them together.\n",
    "The rows that were selected for this process is then dropped from the original frame(s) and the aggregated rows are added back.\n",
    "'''\n",
    "mask=io[\"DELETE\"].str.contains(\"cons_hh\")\n",
    "mask_ene = io_energy[\"DELETE\"].str.contains(\"cons_hh\")\n",
    "io_agg = io[mask].groupby(io.columns.difference([\"DELETE\",\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "io_energy_agg= io_energy[mask_ene].groupby(io_energy.columns.difference([\"DELETE\",\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "'''drop rows that had been aggregated'''\n",
    "io = io[~mask]\n",
    "io_energy=io_energy[~mask_ene]\n",
    "'''drop DELETE'''\n",
    "io.drop(columns=['DELETE'],inplace=True)\n",
    "io_energy.drop(columns=['DELETE'],inplace=True)\n",
    "'''reconcat'''\n",
    "io=pd.concat([io,io_agg])\n",
    "io_energy=pd.concat([io_energy,io_energy_agg])\n",
    "'''add t's'''\n",
    "io=pd.concat([io,pd.DataFrame(columns=['t'],index=io.index,data=2020)],axis=1)\n",
    "io_energy=pd.concat([io_energy,pd.DataFrame(columns=['t'],index=io_energy.index,data=2020)],axis=1)\n",
    "'''reorder for consistency with GR-variables'''\n",
    "io=io[['row_l1','i', 'd', 't', 'level']]\n",
    "io_energy=io_energy[['row_l1','i', 'd', 't', 'level']]\n",
    "'''disentagle'''\n",
    "io_y=io[io['row_l1']=='production']\n",
    "io_m=io[io['row_l1']=='import']\n",
    "io_a=io[io['row_l1']=='prim_input']\n",
    "io_ene_y=io_energy[io_energy['row_l1']=='production']\n",
    "io_ene_m=io_energy[io_energy['row_l1']=='import']\n",
    "io_ene_a=io_energy[io_energy['row_l1']=='prim_input']\n",
    "'''drop columns'''\n",
    "io_y.drop(columns=['row_l1'],inplace=True)\n",
    "io_m.drop(columns=['row_l1'],inplace=True)\n",
    "io_ene_a.drop(columns=['row_l1'],inplace=True)\n",
    "io_ene_y.drop(columns=['row_l1'],inplace=True)\n",
    "io_ene_m.drop(columns=['row_l1'],inplace=True)\n",
    "io_a.drop(columns=['row_l1'],inplace=True)\n",
    "'''apply a_dict'''\n",
    "io_a['i'].replace(dict_a,inplace=True)\n",
    "io_ene_a['i'].replace(dict_a,inplace=True)\n",
    "io_a=io_a.groupby(['d','i','t'],as_index=False).agg({'level':'sum'})\n",
    "io_combined_a = pd.concat([io_a, io_ene_a]).groupby(io_a.columns.difference([\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "'''add energy to non-energy for vIO_{y,m,a}'''\n",
    "io_combined_y = pd.concat([io_y, io_ene_y]).groupby(io_y.columns.difference([\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "io_combined_m=pd.concat([io_m, io_ene_m]).groupby(io_m.columns.difference([\"level\"]).tolist(), as_index=False)[\"level\"].sum()\n",
    "io_combined_a=io_combined_a.groupby(['d','i','t'],as_index=False).agg({'level':'sum'})\n",
    "'''initialize sets'''\n",
    "i=gp.Set(m,name='i',description='sectors',domain_forwarding=True)\n",
    "a_rows_=gp.Set(m,'a_rows_',description='other rows in the input-output table',domain_forwarding=True)\n",
    "'''add tot'''\n",
    "io_y_agg=io_y.groupby('d',as_index=False)['level'].sum()\n",
    "io_y_agg.insert(0,'i','tot')\n",
    "io_y_agg.insert(2,'t','2020')\n",
    "io_y=pd.concat([io_y,io_y_agg])\n",
    "io_m_agg=io_m.groupby('d',as_index=False)['level'].sum()\n",
    "io_m_agg.insert(0,'i','tot')\n",
    "io_m_agg.insert(2,'t','2020')\n",
    "io_m=pd.concat([io_m,io_m_agg])\n",
    "io_a_agg=io_a.groupby('d',as_index=False)['level'].sum()\n",
    "io_a_agg.insert(0,'i','tot')\n",
    "io_a_agg.insert(2,'t','2020')\n",
    "io_a=pd.concat([io_a,io_a_agg])\n",
    "io_combined_y_agg=io_combined_y.groupby('d',as_index=False)['level'].sum()\n",
    "io_combined_y_agg.insert(0,'i','tot')\n",
    "io_combined_y_agg.insert(2,'t','2020')\n",
    "io_combined_y=pd.concat([io_combined_y,io_combined_y_agg])\n",
    "io_combined_m_agg=io_combined_m.groupby('d',as_index=False)['level'].sum()\n",
    "io_combined_m_agg.insert(0,'i','tot')\n",
    "io_combined_m_agg.insert(2,'t','2020')\n",
    "io_combined_m=pd.concat([io_combined_m,io_combined_m_agg])\n",
    "'''change order for GR-compatibility'''\n",
    "io_a=io_a[['i','d','t','level']]\n",
    "io_combined_a=io_combined_a[['i','d','t','level']]\n",
    "\n",
    "#export\n",
    "vIOxE_y=gp.Parameter(m,name='vIOxE_y',domain=[i,d,t],description='HEY. non-energy IO of domestic production',records=io_y.values.tolist(),domain_forwarding=True)\n",
    "vIOxE_m=gp.Parameter(m,name='vIOxE_m',domain=[i,d,t],description='HEY. non-energy IO of imports',records=io_m.values.tolist(),domain_forwarding=True)\n",
    "\n",
    "vIO_y=gp.Parameter(m,name='vIO_y',domain=[d,i,t],description='.HEY Production IO',records=io_combined_y.values.tolist(),domain_forwarding=True)\n",
    "vIO_m=gp.Parameter(m,name='vIO_m',domain=[d,i,t],description='HEY. Production IO',records=io_combined_m.values.tolist(),domain_forwarding=True)\n",
    "\n",
    "vIO_a=gp.Parameter(m,name='vIO_a',domain=[a_rows_,d,t],description='HEY. other IO',records=io_combined_a.values.tolist(),domain_forwarding=True)\n",
    "vIOxE_a=gp.Parameter(m,name='vIOxE_a',domain=[a_rows_,d,t],description='HEY. non energy other IO',records=io_a.values.tolist(),domain_forwarding=True)\n",
    "\n",
    "'''IO investment from here'''\n",
    "\n",
    "io_inv=pd.read_excel(r'data\\io_invest_long_format.xlsx',keep_default_na=True)\n",
    "io_inv.rename(columns={'col':'i','invest_group':'k','value':'level'},inplace=True)\n",
    "io_inv_dict={'invest_build':'iB','invest_other':'iM','invest_trans':'iT'}\n",
    "'''apply dict for GR-compatible codes'''\n",
    "io_inv['k']=io_inv['k'].replace(io_inv_dict)\n",
    "\n",
    "'''Define set for capital types'''\n",
    "k=gp.Set(m,name=\"k\",description='HEY. capital types',domain_forwarding=True)\n",
    "\n",
    "'''atm we do not care abt. \"sender\" of capital, just building qI_k_i'''\n",
    "io_inv_qI_k_i=io_inv.copy(deep=True)\n",
    "io_inv_qI_k_i.drop(columns=['row_l1','row_l2'],inplace=True)\n",
    "io_inv_qI_k_i=pd.concat([io_inv_qI_k_i,pd.DataFrame(columns=(['t']),index=io_inv_qI_k_i.index,data=2020)],axis=1)\n",
    "io_inv_qI_k_i=io_inv_qI_k_i[['k','i','t','level']]\n",
    "'''aggregate'''\n",
    "io_inv_qI_k_i_agg=io_inv_qI_k_i.groupby(['k','i','t'],as_index=False).agg({'level':'sum'})\n",
    "#send it\n",
    "'''Note: There are many sectors here that are not represented in the other IO-tables, should be brought up to date'''\n",
    "#qI_k_i=gp.Parameter(m,'qI_k_i',domain=[k,i,t],description='hEY. Real capital stock by capital type and industry',records=io_inv_qI_k_i_agg.values.tolist(),domain_forwarding=True)\n",
    "\n",
    "#i_=gp.Alias(m,name='i_',alias_with=i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demand-side based io incl. employee compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\1597611272.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_qRxE.rename(columns={'col_l2':'i','value':'level'},inplace=True)\n",
      "C:\\Users\\B247549\\AppData\\Local\\Temp\\ipykernel_12948\\1597611272.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  io_qRxE['row_l2']=io_qRxE['row_l2'].replace(dict_a)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''build qL'''\n",
    "io_qRxE=io_forlater[io_forlater['col_l1']=='cons_inter']\n",
    "io_qRxE.rename(columns={'col_l2':'i','value':'level'},inplace=True)\n",
    "\n",
    "io_qRxE['row_l2']=io_qRxE['row_l2'].replace(dict_a)\n",
    "\n",
    "io_l=io_qRxE[io_qRxE['row_l2']=='SalEmpl']\n",
    "io_l_s=io_l.groupby(['i','year'],as_index=False).agg({'level':'sum'})\n",
    "io_l_s=gp.Parameter(m,name='qL',domain=[i,t],description='HEY. Wage expenses',records=io_l_s.values.tolist(),domain_forwarding=True)\n",
    "'''nemployed from here'''\n",
    "employed=pd.read_excel(r'data\\employed.xlsx',keep_default_na=True)\n",
    "nemployed_frame = pd.DataFrame(columns=['t', 'level'], data=[[2020, employed['employed'].sum()]])\n",
    "nemployed=gp.Parameter(m,name='nEmployed',domain=[t],description='HEY. Total number of employees: Mangler selvstændige',records=nemployed_frame.values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for capital transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_assets=pd.read_excel(r'data\\fixed_assets.xlsx',keep_default_na=True)\n",
    "\n",
    "m.write('dataa_ny.gdx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
